model1:
  class_name: ddm.ddm_const_ode_2.LatentDiffusion
  image_size: [256, 256]
  ckpt_path:
  ignore_keys: []
  only_model: False
  sampling_timesteps: 100
  loss_type: l2
  start_dist: normal
  perceptual_weight: 1
  scale_factor: 0.165
  scale_by_std: True
  default_scale: True
  scale_by_softsign: False
  eps: !!float 1e-3
  sigma_max: 1
  sigma_min: 0.001
  ldm: True
  weighting_loss: True
  use_disloss: False
  use_l1: False
  use_augment: False
  first_stage:  ########################################### cat ############################################
    class_name: ddm.encoder_decoder.AutoencoderKL
    embed_dim: 3
    lossconfig:
      disc_start: 20001
      kl_weight: 0.000001
      disc_weight: 0.5
    ddconfig:
      double_z: True
      z_channels: 3
      resolution: [ 256, 256 ]
      in_channels: 3
      out_ch: 3
      ch: 128
      ch_mult: [ 1,2,4 ]  # num_down = len(ch_mult)-1
      num_res_blocks: 2
      attn_resolutions: [ ]
      dropout: 0.0
    ckpt_path: "/data1/zoushilong/afhqv2/results_ae_kl_256x256_d4_wild/model-10.pt"
  unet:
    class_name: unet.uncond_unet_sd_2.EDMPrecond
    img_resolution: 64
    img_channels: 3
    sigma_data: 1.0  # Expected standard deviation of the training data.
    model_type: 'DhariwalUNet'
    model_channels: 128  # Base multiplier for the number of channels.
    channel_mult: [ 1, 2, 2, 2 ]
    channel_mult_emb: 4  # Multiplier for the dimensionality of the embedding vector.
    num_blocks: 3  # Number of residual blocks per resolution.
    attn_resolutions: [ 16, 8 ]  # List of resolutions with self-attention.
    dropout: 0.1  # dropout.
    label_dropout: 0
    augment_dim: 0


model2:
  class_name: ddm.ddm_const_ode_2.LatentDiffusion
  image_size: [256, 256]
  ckpt_path:
  ignore_keys: []
  only_model: False
  sampling_timesteps: 100
  loss_type: l2
  start_dist: normal
  perceptual_weight: 1
  scale_factor: 0.165
  scale_by_std: True
  default_scale: True
  scale_by_softsign: False
  eps: !!float 1e-3
  sigma_max: 1
  sigma_min: 0.001
  ldm: True
  weighting_loss: True
  use_disloss: False
  use_l1: False
  use_augment: False
  first_stage:  ########################################### dog ############################################
    class_name: ddm.encoder_decoder.AutoencoderKL
    embed_dim: 3
    lossconfig:
      disc_start: 20001
      kl_weight: 0.000001
      disc_weight: 0.5
    ddconfig:
      double_z: True
      z_channels: 3
      resolution: [ 256, 256 ]
      in_channels: 3
      out_ch: 3
      ch: 128
      ch_mult: [ 1,2,4 ]  # num_down = len(ch_mult)-1
      num_res_blocks: 2
      attn_resolutions: [ ]
      dropout: 0.0
    ckpt_path: "/data1/zoushilong/afhqv2/results_ae_kl_256x256_d4_dog/model-10.pt"
  unet:
    class_name: unet.uncond_unet_sd_2.EDMPrecond
    img_resolution: 64
    img_channels: 3
    sigma_data: 1.0  # Expected standard deviation of the training data.
    model_type: 'DhariwalUNet'
    model_channels: 128  # Base multiplier for the number of channels.
    channel_mult: [ 1, 2, 2, 2 ]
    channel_mult_emb: 4  # Multiplier for the dimensionality of the embedding vector.
    num_blocks: 3  # Number of residual blocks per resolution.
    attn_resolutions: [ 16, 8 ]  # List of resolutions with self-attention.
    dropout: 0.1  # dropout.
    label_dropout: 0
    augment_dim: 0

net_G:
  class_name: ddm.cycle_generator_2.ResnetGenerator_timestep_restime_2_attn
  input_nc: 3
  output_nc: 3
  ngf: 64
  dim: 128
  n_blocks: 12

net_D:
  class_name: ddm.cycle_discriminator.NLayerDiscriminator2
  input_nc: 3
  ndf: 64

data:
  class_name: ddm.data.RGB_Unpair_dataset
  source_folder_name: 'wild'
  target_folder_name: 'dog'
  split: train
  image_size: [ 286, 286 ]
  data_root: "/data1/zoushilong/afhqv2"
  augment_horizontal_flip: True
  batch_size: 24
  num_workers: 4

trainer:
  gradient_accumulate_every: 2
  lr: !!float 1e-5
  trans_net_lr: !!float 2e-4
  min_lr: !!float 5e-6
  train_num_steps: 200000
  save_every: 5000
  sample_every: 5000
  log_freq: 500
  results_folder: "/data1/zoushilong/afhqv2/train_cycle_C_disc_G_blocks_12"
  amp: False
  fp16: False
  resume_milestone: 0
  test_before: True
  ema_update_after_step: 20000
  ema_update_every: 8
  ft_use_ema: True
  ckpt_path1: "/data1/zoushilong/afhqv2/results_ddm_const_uncond_unet_ldm_wild_ode/model-27.pt"
  ckpt_path2: "/data1/zoushilong/afhqv2/results_ddm_const_uncond_unet_ldm_dog_ode/model-39.pt"
  # for mcl loss
  temp: 0.1
  mcl_weight: 0.01
  # for perceptual loss
  perceptual_weight: 0.5
  # for cycle losss
  cycle_weight: 8
  # for identity loss
  idt_weight: 0.5
  # for fine-tune diffusion model
  ft_weight: 0.05
  # cosine_similarity: 0.1

data_test:
  class_name: ddm.data.Single_dataset
  datafolder_name: 'wild'
  split: test
  image_size: [ 256, 256 ]
  data_root: "/data1/zoushilong/afhqv2"
  augment_horizontal_flip: True
  num_workers: 4

data_test2:
  class_name: ddm.data.Single_dataset
  datafolder_name: 'dog'
  split: test
  image_size: [ 256, 256 ]
  data_root: "/data1/zoushilong/afhqv2"
  augment_horizontal_flip: True
  num_workers: 4

sampler:
  split: test
  batch_size: 16
  use_ema: True
  test_in_train: False
  cal_metrics: True

  task: "wild2dog"
  source_gt_path: '/data1/zoushilong/afhqv2/test/wild'
  target_gt_path: '/data1/zoushilong/afhqv2/test/dog'

  # task: "dog2wild"
  # source_gt_path: '/data1/zoushilong/afhqv2/test/dog'
  # target_gt_path: '/data1/zoushilong/afhqv2/test/wild'

  ckpt_path: "/data1/zoushilong/afhqv2/train_cycle_C_disc_G_blocks_12/model-12.pt"
  save_folder: "/data1/zoushilong/afhqv2/sample_results/wild2dog_model_12_timestep100"
